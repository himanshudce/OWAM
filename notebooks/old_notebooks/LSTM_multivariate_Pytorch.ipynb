{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import time\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pickle\n",
    "pd.set_option('display.max_rows', 500)\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from math import sqrt\n",
    "# import rmse from sklearn\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "# define random seeds for Neural Networks\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)\n",
    "# ignore warnings jupyter notebook\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OWRI FRAMEWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results save path\n",
    "base_result_path = '../results/METR-LA/LSTM'\n",
    "exp_name = 'multivariate_AE_weighted_vector_real_time_results.pkl'\n",
    "results_save_path = os.path.join(base_result_path, exp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/METR-LA/METR_OWRI/featured_fpds_raw.pickle', 'rb') as f:\n",
    "    results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data of correlated results from pickle file\n",
    "with open('../results/METR-LA/outlier_scores/AE/correlated_results.pickle', 'rb') as f:\n",
    "    correlated_results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_trejectory_data(results, trajectory, direction):\n",
    "    data = pd.DataFrame()\n",
    "    for intersection_name in results[trajectory][direction]['raw']:\n",
    "        intersection = results[trajectory][direction]['raw'][intersection_name]\n",
    "        intersection = intersection.rename(columns={\"cars\": intersection_name})\n",
    "        intersection = intersection.set_index(pd.DatetimeIndex(intersection['timestamp']))\n",
    "        intersection = intersection.drop(columns=['timestamp'])\n",
    "        data = pd.merge(data, intersection, left_index=True, right_index=True, how='outer')\n",
    "    data.dropna(inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_trejectory_data_metr(results):\n",
    "    data = pd.DataFrame()\n",
    "    for intersection_name in results['raw']:\n",
    "        intersection = results['raw'][intersection_name]\n",
    "        intersection = intersection.rename(columns={\"cars\": intersection_name})\n",
    "        intersection = intersection.set_index(pd.DatetimeIndex(intersection['timestamp']))\n",
    "        intersection = intersection.drop(columns=['timestamp'])\n",
    "        data = pd.merge(data, intersection, left_index=True, right_index=True, how='outer')\n",
    "    data.dropna(inplace=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_top_corr_features(data, top_corr_df):\n",
    "    # using numpy broadcasting\n",
    "    corr_array = top_corr_df.values\n",
    "    corr_array = corr_array.reshape(1,1,corr_array.shape[0],corr_array.shape[1])\n",
    "    \n",
    "\n",
    "\n",
    "    # using loops\n",
    "    # arr1 = []\n",
    "    # for i in range(data.shape[0]):\n",
    "    #     arr2 = []\n",
    "    #     for j in range(data.shape[1]):\n",
    "    #         temp = data[i][j].reshape(-1,1) * top_corr_df.values\n",
    "    #         temp = temp.reshape(-1)\n",
    "    #         arr2.append(temp)\n",
    "    #     arr1.append(np.array(arr2))\n",
    "    # return np.array(arr1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df(df,top_corr_df, n_obs, n_features, sequence_length):\n",
    "    #do scaling:\n",
    "    scaler = StandardScaler()\n",
    "    train_portion = 0.8\n",
    "    test_portion = 0.2\n",
    "    df_train = df[:math.ceil(len(df)*train_portion)].values\n",
    "    df_test = df[math.ceil(len(df)*(train_portion)):].values\n",
    "    train_X, train_y = df_train[:, :n_obs], df_train[:, -n_features]\n",
    "    test_X, test_y = df_test[:, :n_obs], df_test[:, -n_features]\n",
    "    # scl = scaler.fit(train_X) # fit only on training data\n",
    "    # train_X = scl.transform(train_X)\n",
    "    # test_X = scl.transform(test_X)\n",
    "    train_X = train_X.reshape((train_X.shape[0], sequence_length, n_features))\n",
    "    test_X = test_X.reshape((test_X.shape[0], sequence_length, n_features))\n",
    "\n",
    "    # # add top correlated features weighted sum\n",
    "    # train_X = add_top_corr_features(train_X, top_corr_df)\n",
    "    # test_X = add_top_corr_features(test_X, top_corr_df)\n",
    "\n",
    "\n",
    "    # # add top correlated features as it is \n",
    "    # corr_array = top_corr_df.values\n",
    "    # corr_tiled_train = np.tile(corr_array, (train_X.shape[0], sequence_length, 1, 1))\n",
    "    # corr_tiled_test = np.tile(corr_array, (test_X.shape[0], sequence_length, 1, 1))\n",
    "    # train_X = np.concatenate([train_X[:, :, :, np.newaxis], corr_tiled_train], axis=3) # add outlier dimension\n",
    "    # train_X = train_X.reshape(train_X.shape[0],train_X.shape[1],-1) # reshape to 3D\n",
    "    # test_X = np.concatenate([test_X[:, :, :, np.newaxis], corr_tiled_test], axis=3) # add outlier dimension\n",
    "    # test_X = test_X.reshape(test_X.shape[0],test_X.shape[1],-1) # reshape to 3D\n",
    "\n",
    "    return train_X, train_y, test_X, test_y, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_df(multivariate_dict):\n",
    "    train_X = c = np.stack(multivariate_dict['train_X'], axis=-2)\n",
    "    train_y = c = np.stack(multivariate_dict['train_y'], axis=1)\n",
    "    test_X = c = np.stack(multivariate_dict['test_X'], axis=-2)\n",
    "    test_y = c = np.stack(multivariate_dict['test_y'], axis=1)\n",
    "    train_X = train_X.reshape(train_X.shape[0],train_X.shape[1],-1) # reshape to 3D\n",
    "    test_X = test_X.reshape(test_X.shape[0],test_X.shape[1],-1) # reshape to 3D\n",
    "    return train_X, train_y, test_X, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_df(train_X,test_X):\n",
    "    scaler = StandardScaler()\n",
    "    train_X = scaler.fit_transform(train_X.reshape(-1, train_X.shape[-1])).reshape(train_X.shape)\n",
    "    test_X = scaler.transform(test_X.reshape(-1, test_X.shape[-1])).reshape(test_X.shape)\n",
    "    return train_X, test_X, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
    "\tdf = pd.DataFrame(data)\n",
    "\tcols, names = list(), list()\n",
    "\t# input sequence (t-n, ... t-1)\n",
    "\tfor i in range(n_in, 0, -1):\n",
    "\t\tcols.append(df.shift(i))\n",
    "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# forecast sequence (t, t+1, ... t+n)\n",
    "\tfor i in range(0, n_out):\n",
    "\t\tcols.append(df.shift(-i))\n",
    "\t\tif i == 0:\n",
    "\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "\t\telse:\n",
    "\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "\t# put it all together\n",
    "\tagg = pd.concat(cols, axis=1)\n",
    "\tagg.columns = names\n",
    "\t# drop rows with NaN values\n",
    "\tif dropnan:\n",
    "\t\tagg.dropna(inplace=True)\n",
    "\treturn agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_corr_features(top_corr_df, top_k_col):\n",
    "    # create new coorelation df with top k correlated intersections for each intersection\n",
    "    new_corr_df = []\n",
    "    for col in top_corr_df.columns:\n",
    "        new_corr_df.append(top_corr_df[col].sort_values(ascending=False)[:top_k_col].values)\n",
    "    new_corr_df = pd.DataFrame(new_corr_df, index=top_corr_df.columns)\n",
    "    return new_corr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_uni(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, device = 'mps',layer_dim=1, dropout_prob = 0.2):\n",
    "        super(LSTM_uni, self).__init__()\n",
    "        self.hidden_dim = hidden_dim # number of hidden units in hidden state\n",
    "        self.layer_dim = layer_dim # number of stacked lstm layers\n",
    "        # batch_first=True causes input/output tensors to be of shape\n",
    "        # (batch_dim, seq_dim, feature_dim)\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True, dropout=dropout_prob)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim) # fully connected layer\n",
    "\n",
    "    def forward(self, x, future=False):\n",
    "        # input x is expected to be of shape (batch_dim, seq_dim, feature_dim)\n",
    "        # hidden and cell states are expected along with input x in LSTMs = (h_0, c_0)\n",
    "        # Initialize hidden state with zeros (layer_dim, batch_size, hidden_dim)\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim, device=device).requires_grad_()\n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim, device=device).requires_grad_()\n",
    "        # LSTM output is Outputs: output, (h_n, c_n)\n",
    "        # output is of shape (batch_dim, seq_dim, hidden_dim), h_n and c_n are of shape (layer_dim, batch_dim, hidden_dim)\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "        out = out[:, -1, :] # only take the last output of the sequence\n",
    "        out = self.fc(out) # fully connected layer\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_X,train_y, loss_fn, optimiser, device, batch_size, epochs=250):\n",
    "    history = {}\n",
    "    history['train_loss'] = []\n",
    "\n",
    "    train_X_loader = DataLoader(train_X, batch_size=batch_size, shuffle=False)\n",
    "    train_y_loader = DataLoader(train_y, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        history[epoch] = []\n",
    "        ep_start = time.time()\n",
    "        running_loss = 0.0\n",
    "        for bx, data in enumerate(zip(train_X_loader,train_y_loader)):\n",
    "            X = data[0].to(device)\n",
    "            y = data[1].to(device)\n",
    "            bt = model(X)\n",
    "            loss = loss_fn(bt.reshape(-1), y.reshape(-1)) # calculate loss for input and recreated output\n",
    "            history[epoch].append(loss.item())\n",
    "            optimiser.zero_grad()\n",
    "            loss.backward()\n",
    "            optimiser.step()\n",
    "            running_loss += loss.item()\n",
    "        epoch_loss = running_loss/train_X.shape[0]\n",
    "        history['train_loss'].append(epoch_loss)\n",
    "\n",
    "    return history\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluation( model, test_X, device):\n",
    "    test_X_loader = DataLoader(test_X, batch_size=64, shuffle=False)\n",
    "    model = model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for bx, data in enumerate(test_X_loader):\n",
    "            X = data.to(device)\n",
    "            bt = model(X)\n",
    "            preds.append(bt.cpu().numpy())\n",
    "    preds = np.vstack(preds)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_df_2(df,ae_score):\n",
    "    weighted_ls = []\n",
    "    correlated_AE = ae_score.corr().values\n",
    "    for idx, row in df.iterrows():\n",
    "        weighted_ls.append(np.sum(row.values*correlated_AE, axis=1))\n",
    "    weighted_df = pd.DataFrame(weighted_ls, columns=df.columns)\n",
    "    return weighted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare variables\n",
    "# thresholds = [0,0.25,0.5,0.75,1]\n",
    "thresholds = [0, 0.05, 0.1, 0.15]\n",
    "epoch = 100\n",
    "batch_size = 64\n",
    "learning_rate = 0.1\n",
    "hidden_size = 32\n",
    "num_layers = 1\n",
    "dropout = 0.2\n",
    "sequence_length = 12\n",
    "output_pred = 1 # number of time steps to predict\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors={}  # define a dictionary to store the errors\n",
    "dfs={} # Define a dictionary to store the dataframes\n",
    "errors = {} # Define a dictionary to store the errors\n",
    "intersection_arrays = [] # Define a list to store the intersection arrays\n",
    "for threshold in thresholds:\n",
    "    print(\"Starting threshold: {}\".format(threshold))\n",
    "    # Define a dictionary to store the errors for this trajectory, direction, and threshold\n",
    "    errors[threshold]={}\n",
    "    # ------------------------------------ data processing ---------------------------------------- #\n",
    "    ae_score = correlated_results['df'] # AE scores of the current trajectory and direction\n",
    "    df = merge_trejectory_data_metr(results)# get raw data of the current trajectory and direction\n",
    "    df = df[ae_score.columns.to_list()]\n",
    "    number_of_cols = math.ceil(len(ae_score.columns)*threshold) # number of outlier weighted intersections to use\n",
    "    if number_of_cols==0: # if threshold is 0, then use the target intersection only\n",
    "        number_of_cols=1\n",
    "    \n",
    "    multivariate_dict = {\n",
    "        'train_X':[],'train_y':[],'test_X':[],'test_y':[]\n",
    "    }\n",
    "    for target in df.columns:\n",
    "        top_corr_df = ae_score.corr()[target].sort_values(ascending=False)[:number_of_cols] # get the top correlated intersections\n",
    "        isct_inc = top_corr_df.index.tolist()\n",
    "        df_temp = df[isct_inc].copy(deep=True)\n",
    "        df_temp = df_temp.mul(top_corr_df, axis=1)\n",
    "        df_temp = df_temp.astype('float32')\n",
    "        sequence_length = 12 # number of time steps to look back\n",
    "        n_features = len(isct_inc) # number of features (correlated intersections)\n",
    "        output_pred = 1 # number of time steps to predict\n",
    "        n_obs = sequence_length * n_features # number of columns in the input\n",
    "        reframed = series_to_supervised(df_temp, sequence_length, output_pred)\n",
    "        train_X, train_y, test_X, test_y, scl = preprocess_df(reframed, top_corr_df, n_obs, n_features, sequence_length)\n",
    "        train_X, train_y, test_X, test_y = train_X.astype('float32'), train_y.astype('float32'), test_X.astype('float32'), test_y.astype('float32')\n",
    "        multivariate_dict['train_X'].append(train_X)\n",
    "        multivariate_dict['train_y'].append(train_y)\n",
    "        multivariate_dict['test_X'].append(test_X)\n",
    "        multivariate_dict['test_y'].append(test_y)\n",
    "    \n",
    "    # reshape the data for training and testing\n",
    "    train_X, train_y, test_X, test_y = reshape_df(multivariate_dict)\n",
    "    del multivariate_dict # delete the dictionary to save memory\n",
    "    train_X, test_X, scaler = scale_df(train_X, test_X)\n",
    "\n",
    "    # # # ------------------------------------ modelling ---------------------------------------------- #\n",
    "    # define model, loss function and optimizer\n",
    "    model = LSTM_uni(input_dim = train_X.shape[-1], hidden_dim = hidden_size, layer_dim = num_layers, output_dim = train_y.shape[1], dropout_prob= dropout)\n",
    "    model = model.to(device)\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    "    optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    start = time.time()\n",
    "    history = train_model(model, train_X,train_y, loss_fn, optimiser, device, batch_size = batch_size, epochs=epoch)\n",
    "    end = time.time()\n",
    "    print(\"Training time: {}\".format(end-start))\n",
    "\n",
    "    # ------------------------------------ evaluation ---------------------------------------------- #\n",
    "    yhat = model_evaluation(model, test_X , device)\n",
    "    errors[threshold]['RMSE'] = sqrt(mean_squared_error(yhat,test_y))\n",
    "    errors[threshold]['MAE'] = mean_absolute_error(yhat,test_y)\n",
    "    errors[threshold]['history'] = history\n",
    "    errors[threshold]['df'] = {\"Real\":test_y,\"Predicted\":yhat}\n",
    "    errors[threshold]['train_time'] = end-start\n",
    "    print(\"RMSE: {}\".format(errors[threshold]['RMSE']))\n",
    "\n",
    "\n",
    "# save errors in save path as pickle file\n",
    "with open(results_save_path, 'wb') as handle:\n",
    "    pickle.dump(errors, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# errors={}  # define a dictionary to store the errors\n",
    "# dfs={} # Define a dictionary to store the dataframes\n",
    "# errors = {} # Define a dictionary to store the errors\n",
    "# intersection_arrays = [] # Define a list to store the intersection arrays\n",
    "# for trajectory in results.keys(): # Loop over all trajectories\n",
    "#     print(\"\\nStarting trajectory: {}\".format(trajectory))  \n",
    "#     # Define a dictionary to store the errors for this trajectory\n",
    "#     errors[trajectory]={}\n",
    "#     # Loop over all directions\n",
    "#     for direction in results[trajectory]:\n",
    "#         print(\"\\nStarting direction: {}\".format(direction))\n",
    "#         # Define a dictionary to store the errors for this trajectory and direction\n",
    "#         errors[trajectory][direction]={}\n",
    "#         # Loop over all thresholds\n",
    "#         for threshold in thresholds:\n",
    "#             print(\"Starting threshold: {}\".format(threshold))\n",
    "#             # Define a dictionary to store the errors for this trajectory, direction, and threshold\n",
    "#             errors[trajectory][direction][threshold]={}\n",
    "#             # ------------------------------------ data processing ---------------------------------------- #\n",
    "#             df = merge_trejectory_data(results, trajectory, direction)# get raw data of the current trajectory and direction\n",
    "#             ae_score = correlated_results[trajectory][direction] # AE scores of the current trajectory and direction\n",
    "#             top_k_col = math.ceil(len(ae_score.columns)*threshold) # number of outlier weighted intersections to use\n",
    "#             if top_k_col==0: # if threshold is 0, then use the target intersection only\n",
    "#                 top_k_col=1\n",
    "#             top_corr_df = ae_score.corr()[df.columns.to_list()] # rearrange the correlation matrix\n",
    "#             top_corr_df = get_top_corr_features(top_corr_df, top_k_col) # get the top k correlated intersections\n",
    "#             n_features = len(df.columns) # number of features (correlated intersections)\n",
    "#             n_obs = sequence_length * n_features # number of columns in the input\n",
    "#             # weighted_df = preprocess_df_2(df,ae_score)\n",
    "#             reframed = series_to_supervised(df, sequence_length, output_pred)\n",
    "#             train_X, train_y, test_X, test_y, scl = preprocess_df(reframed,top_corr_df, n_obs, n_features, sequence_length)\n",
    "#             train_X, train_y, test_X, test_y = train_X.astype('float32'), train_y.astype('float32'), test_X.astype('float32'), test_y.astype('float32')\n",
    "\n",
    "\n",
    "#             # # # ------------------------------------ modelling ---------------------------------------------- #\n",
    "#             # define model, loss function and optimizer\n",
    "#             model = LSTM_uni(input_dim = train_X.shape[-1], hidden_dim = hidden_size, layer_dim = num_layers, output_dim = train_y.shape[1], dropout_prob= dropout)\n",
    "#             model = model.to(device)\n",
    "#             loss_fn = torch.nn.MSELoss()\n",
    "#             optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#             start = time.time()\n",
    "#             history = train_model(model, train_X,train_y, loss_fn, optimiser, device, batch_size = batch_size, epochs=epoch)\n",
    "#             end = time.time()\n",
    "#             print(\"Training time: {}\".format(end-start))\n",
    "\n",
    "#             # ------------------------------------ evaluation ---------------------------------------------- #\n",
    "#             yhat = model_evaluation(model, test_X , device)\n",
    "#             errors[trajectory][direction][threshold]['RMSE'] = sqrt(mean_squared_error(yhat,test_y))\n",
    "#             errors[trajectory][direction][threshold]['MAE'] = mean_absolute_error(yhat,test_y)\n",
    "#             errors[trajectory][direction][threshold]['history'] = history\n",
    "#             errors[trajectory][direction][threshold]['df'] = {\"Real\":test_y,\"Predicted\":yhat}\n",
    "#             errors[trajectory][direction][threshold]['train_time'] = end-start\n",
    "#             print(\"RMSE: {}\".format(errors[trajectory][direction][threshold]['RMSE']))\n",
    "\n",
    "\n",
    "# # save errors in save path as pickle file\n",
    "# with open(results_save_path, 'wb') as handle:\n",
    "#     pickle.dump(errors, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multivariate_dict['769346']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_score.corr().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort values by each column and get the top k and keep ramaining columns as NaN\n",
    "def get_top_corr_features(df, k):\n",
    "    df = df.apply(lambda x: x.sort_values(ascending=False).index)\n",
    "    df = df.apply(lambda x: pd.Series(x[:k]))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = ae_score.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.apply(lambda x: x.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['717508'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df = df_temp.apply(lambda row: row.sort_values(ascending=False).iloc[:10]).replace(np.nan, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in df_temp.iterrows():\n",
    "    l = row.sort_values(ascending=False).iloc[:10].index.to_list()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "60.125*0.698464"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = train_X[0][0] * df_temp.values\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.T['717508'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df['717508'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for idx, row in df.iterrows():\n",
    "    temp = row.sort_values(ascending=False)\n",
    "    temp.iloc[2:] = 0\n",
    "    print(temp)\n",
    "    c+=1\n",
    "    if c==3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df['772151']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[ae_score.corr().columns.to_list()].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = top_corr_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_2 = temp.reshape(1,1,temp.shape[0],temp.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp3 = train_X[:, :, :, np.newaxis] * temp_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp4 = temp3.reshape(temp3.shape[0],temp3.shape[1],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data of correlated results from pickle file\n",
    "with open('../results/hauge/LSTM/', 'rb') as f:\n",
    "    errors = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AE_results={}\n",
    "for trajectory in errors.keys():\n",
    "    for direction in errors[trajectory].keys():\n",
    "        for threshold in errors[trajectory][direction].keys():\n",
    "            AE_results[trajectory+'_'+direction+'_'+str(threshold)] = errors[trajectory][direction][threshold]['MAE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AE_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AE_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AE_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_corr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_corr_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.reshape(top_corr_df.values, (1, 1, 15, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(ls_big).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(train_X[0][0].reshape(-1,1), top_corr_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_corr_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiplying with the 3rd dimention of the whole train_X with shape (n_samples, n_timesteps, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../results/hauge/LSTM/multivariate_AE_weighted_real_time_results.pkl', 'rb') as f:\n",
    "    errors = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AE_results={}\n",
    "for trajectory in errors.keys():\n",
    "    for direction in errors[trajectory].keys():\n",
    "        for threshold in errors[trajectory][direction].keys():\n",
    "            AE_results[trajectory+'_'+direction+'_'+str(threshold)] = errors[trajectory][direction][threshold]['RMSE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AE_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1 = df.iloc[0:1].values\n",
    "temp1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_corr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_ls = []\n",
    "for idx, row in df.iterrows():\n",
    "    weighted_ls.append(np.sum(row.values*ae_score.corr().values, axis=1))\n",
    "    break\n",
    "weighted_df = pd.DataFrame(weighted_ls, columns=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiply top_corr_df and temp1 on axis 1\n",
    "temp2 = np.multiply(top_corr_df, temp1.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = df.iloc[0:1].values* ae_score.values\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sum of each row in temp\n",
    "temp.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AE_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXTRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = [a.cpu().detach().numpy(),b.cpu().detach().numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.vstack(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = test_y[:64,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_squared_error(bt.cpu().detach().numpy(),y.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn(bt.reshape(-1),y.reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = ae_score.corr().values\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_tiled = np.tile(b, (12, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_tiled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_tiled_temp = np.tile(b, (78327, 12, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_tiled_temp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_tiled_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_tiled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = np.concatenate([train_X[:, :, :, np.newaxis], b_tiled_temp], axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = np.concatenate([a[:, :, np.newaxis], b_reshaped], axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack each row of b with each element of a\n",
    "a[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.hstack((a[0].reshape(-1,1),b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge a and b to make it 12 x 15 x 15 array\n",
    "c = np.concatenate((a,b), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_score.corr().columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_score.corr()[isct_inc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AE_results={}\n",
    "for trajectory in errors.keys():\n",
    "    for direction in errors[trajectory].keys():\n",
    "        for threshold in errors[trajectory][direction].keys():\n",
    "            AE_results[trajectory+'_'+direction+'_'+str(threshold)] = errors[trajectory][direction][threshold]['RMSE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AE_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OWRI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
