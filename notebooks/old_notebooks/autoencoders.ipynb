{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import scipy\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import timedelta\n",
    "\n",
    "# define random seeds for Neural Networks\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# ignore warnings jupyter notebook\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========== FUNCTION =============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare input for AEs, \n",
    "# input is combined list of all intersections (vector -> [fpds + hourofday+ weekofday + intersectoion]) for each week and each hour sorted based on time\n",
    "# output is -> list of feature vector and time for all interctions combined --> [[fpds + hourofday+ weekofday + intersectoion], time]\n",
    "def prepare_input_for_AEs(featured_fpds):\n",
    "    combined_fpds_for_AEs = []\n",
    "    for intersection in featured_fpds.keys():\n",
    "        for i in range(7):\n",
    "            for j in range(24):\n",
    "                for l,m in zip(featured_fpds[intersection][i][j][0],featured_fpds[intersection][i][j][1]):\n",
    "                    combined_fpds_for_AEs.append([l.astype(np.float32),m, intersection])\n",
    "\n",
    "    combined_fpds_for_AEs = sorted(combined_fpds_for_AEs, key=lambda x:x[1])\n",
    "    return np.array(combined_fpds_for_AEs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load data from pickle file\n",
    "def load_pickle(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collate(batch):\n",
    "    # Transpose the batch to get a list of samples\n",
    "    transposed = zip(*batch)\n",
    "    \n",
    "    # Convert each sample to a tensor\n",
    "    tensor_list = [torch.tensor(samples) for samples in transposed]\n",
    "    \n",
    "    # Return the list of tensors\n",
    "    return tensor_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoderDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data[:,0]\n",
    "        self.timestamp = data[:,1]\n",
    "        self.intersection = data[:,2]\n",
    "        self.dim = self.data[0].shape[0]\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        # get integer\n",
    "        my_int = self.data[idx]\n",
    "        \n",
    "        # get numpy datetime64 timestamp\n",
    "        my_timestamp = str(self.timestamp[idx])\n",
    "        \n",
    "        # get string type\n",
    "        my_str = str(self.intersection[idx])\n",
    "        \n",
    "        # return as list\n",
    "        return [my_int, my_timestamp, my_str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(AE, self).__init__()\n",
    "        self.enc = nn.Sequential(\n",
    "            nn.Linear(input_shape, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.dec = nn.Sequential(\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, input_shape),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        encode = self.enc(x)\n",
    "        decode = self.dec(encode)\n",
    "        return decode"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =========== MAIN ==========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from pickle file\n",
    "direction = \"North\"\n",
    "trajectory = \"T1\"\n",
    "load_fpds_path = f\"../data/hauge/processed/featured_fpds_{direction}_{trajectory}.pickle\"\n",
    "featured_fpds = load_pickle(load_fpds_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare input for AEs\n",
    "combined_fpds_for_AEs = prepare_input_for_AEs(featured_fpds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AE_dataset = AutoEncoderDataset(combined_fpds_for_AEs)  # create dataset object\n",
    "AE_dataset.dim # feature vector dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(AE_dataset, batch_size=1, shuffle=False)\n",
    "# train_dataloader = DataLoader(AE_dataset, batch_size=1, shuffle=False, collate_fn=my_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if dataloader is working\n",
    "for i in train_dataloader:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model parameters\n",
    "lr = 1e-2         # learning rate\n",
    "w_d = 1e-5        # weight decay\n",
    "epochs = 1\n",
    "metrics = defaultdict(list)\n",
    "outlier_loss = []\n",
    "outlier_results = []\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AE(AE_dataset.dim)\n",
    "model.to(device)\n",
    "criterion = nn.MSELoss(reduction='mean')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=w_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "start = time.time()\n",
    "for epoch in range(epochs):\n",
    "    ep_start = time.time()\n",
    "    running_loss = 0.0\n",
    "    for bx, data in enumerate(train_dataloader):\n",
    "        bt = data[0]\n",
    "        sensor_time = data[1]\n",
    "        intersection = data[2]     \n",
    "        sample = model(bt.to(device))\n",
    "        loss = criterion(bt.to(device), sample) # calculate loss for input and recreated output\n",
    "        outlier_loss.append(loss.item()) # append loss to list\n",
    "        outlier_results.append([loss.item(),sensor_time, intersection]) # append loss, time and intersection to list\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        # print average loss for every 25% batches\n",
    "        if bx % int(AE_dataset.__len__()/4) == 0:\n",
    "            print('[EPOCH] {}/{}\\t[BATCH] {}/{}\\t[LOSS] {}'.format(epoch+1,epochs,bx+1,AE_dataset.__len__(),running_loss/(bx+1)))\n",
    "    epoch_loss = running_loss/AE_dataset.__len__()\n",
    "    metrics['train_loss'].append(epoch_loss)\n",
    "    ep_end = time.time()\n",
    "    print('-----------------------------------------------')\n",
    "    print('[EPOCH] {}/{}\\n[LOSS] {}'.format(epoch+1,epochs,epoch_loss))\n",
    "    print('Epoch Complete in {}'.format(timedelta(seconds=ep_end-ep_start)))\n",
    "end = time.time()\n",
    "print('-----------------------------------------------')\n",
    "print('[System Complete: {}]'.format(timedelta(seconds=end-start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1,1,figsize=(10,5))\n",
    "ax.set_title('Loss')\n",
    "ax.plot(outlier_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the top 5% of the loss values\n",
    "top_5_percent = np.percentile(outlier_loss, 97)\n",
    "top_5_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_threshold = 0.0\n",
    "upper_threshold = top_5_percent\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.title('Loss Distribution')\n",
    "sns.distplot(outlier_loss,bins=100,kde=True, color='blue')\n",
    "plt.axvline(upper_threshold, 0.0, 10, color='r')\n",
    "plt.axvline(lower_threshold, 0.0, 10, color='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxenplot(outlier_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_df = pd.DataFrame(outlier_results,columns=['loss','timestamp','intersection'])\n",
    "outlier_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_df['timestamp'] = outlier_df['timestamp'].apply(lambda x: x[0])\n",
    "outlier_df['intersection'] = outlier_df['intersection'].apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create seperate columns for each intersection\n",
    "outlier_df_intersection = outlier_df.pivot(index='timestamp', columns='intersection', values='loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(facecolor='w')\n",
    "sns.heatmap(outlier_df_intersection.corr())\n",
    "plt.title(\"Correlations for T1 North\")\n",
    "plt.xlabel('Intersection')\n",
    "plt.ylabel('Intersection')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df = outlier_df_intersection.corr()\n",
    "corr_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments over correlated intersections\n",
    "- Except K198 all intersections are highly correlated\n",
    "- hard to get the intution as the feature vector is - **[fpds + hourofday+ weekofday + intersectoion]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each intersection, get the top 3 correlated intersections\n",
    "top_3_corr = corr_df.apply(lambda x: x.sort_values(ascending=False).index[1:4], axis=1)\n",
    "top_3_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OWRI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
