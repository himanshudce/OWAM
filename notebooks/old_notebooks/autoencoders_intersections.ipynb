{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import scipy\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "import time\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "from datetime import timedelta\n",
    "\n",
    "# define random seeds for Neural Networks\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# ignore warnings jupyter notebook\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# importing from daads\n",
    "import sys\n",
    "sys.path.insert(0, '/Users/himanshu/Master_Thesis/code/OWRI/DAADS')\n",
    "from tools.evaluate import aggregate_dataframe, test_then_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========== FUNCTION =============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare input for AEs, \n",
    "# input is combined list of all intersections (vector -> [fpds + hourofday+ weekofday + intersectoion]) for each week and each hour sorted based on time\n",
    "# output is -> list of feature vector and time for all interctions combined --> [[fpds + hourofday+ weekofday + intersectoion], time]\n",
    "def prepare_input_for_AEs(intersection_data, intersection):\n",
    "    combined_fpds_for_AEs = []\n",
    "    for i in range(7):\n",
    "        for j in range(24):\n",
    "            for fpd,time_instance in zip(intersection_data[i][j][0],intersection_data[i][j][1]):\n",
    "                combined_fpds_for_AEs.append([fpd.astype(np.float32),time_instance, intersection])\n",
    "\n",
    "    combined_fpds_for_AEs = sorted(combined_fpds_for_AEs, key=lambda x:x[1])\n",
    "    return np.array(combined_fpds_for_AEs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare input for AEs, \n",
    "# input is combined list of all intersections (vector -> [fpds + hourofday+ weekofday + intersectoion]) for each week and each hour sorted based on time\n",
    "# output is -> list of feature vector and time for all interctions combined --> [[fpds + hourofday+ weekofday + intersectoion], time]\n",
    "def prepare_input_for_PWAE(intersection_data):\n",
    "    combined_fpds_for_AEs = []\n",
    "    for i in range(7):\n",
    "        for j in range(24):\n",
    "            for fpd,time_instance in zip(intersection_data[i][j][0],intersection_data[i][j][1]):\n",
    "                temp = fpd.astype(np.float32).tolist()\n",
    "                temp.extend([time_instance, 0])\n",
    "                combined_fpds_for_AEs.append(temp)\n",
    "\n",
    "    # combined_fpds_for_AEs = sorted(combined_fpds_for_AEs, key=lambda x:x[12])\n",
    "    return combined_fpds_for_AEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load data from pickle file\n",
    "def load_pickle(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_collate(batch):\n",
    "    # Transpose the batch to get a list of samples\n",
    "    transposed = zip(*batch)\n",
    "    \n",
    "    # Convert each sample to a tensor\n",
    "    tensor_list = [torch.tensor(samples) for samples in transposed]\n",
    "    \n",
    "    # Return the list of tensors\n",
    "    return tensor_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoderDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data[:,0]\n",
    "        self.timestamp = data[:,1]\n",
    "        self.intersection = data[:,2]\n",
    "        self.dim = self.data[0].shape[0]\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        # get integer\n",
    "        my_int = self.data[idx]\n",
    "        \n",
    "        # get numpy datetime64 timestamp\n",
    "        my_timestamp = str(self.timestamp[idx])\n",
    "        \n",
    "        # get string type\n",
    "        my_str = str(self.intersection[idx])\n",
    "        \n",
    "        # return as list\n",
    "        return [my_int, my_timestamp, my_str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE(nn.Module):\n",
    "    def __init__(self, input_shape):\n",
    "        super(AE, self).__init__()\n",
    "        self.enc = nn.Sequential(\n",
    "            nn.Linear(input_shape, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.dec = nn.Sequential(\n",
    "            nn.Linear(16, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, input_shape),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        encode = self.enc(x)\n",
    "        decode = self.dec(encode)\n",
    "        return decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AE_outlier_detection(AE_dataset, device, lr, w_d, epochs, intersection):\n",
    "    model = AE(AE_dataset.dim)\n",
    "    model.to(device)\n",
    "    criterion = nn.MSELoss(reduction='mean')\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=w_d)\n",
    "    train_dataloader = DataLoader(AE_dataset, batch_size=1, shuffle=False)\n",
    "    \n",
    "    \n",
    "    # --------------------- TRAINING ---------------------\n",
    "    # train model\n",
    "    outlier_results = []\n",
    "    start = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for bx, data in enumerate(train_dataloader):\n",
    "            bt = data[0]\n",
    "            sensor_time = data[1][0]\n",
    "            sample = model(bt.to(device))\n",
    "            loss = criterion(bt.to(device), sample) # calculate loss for input and recreated output\n",
    "            outlier_results.append([loss.item(),sensor_time]) # append loss, time and intersection to list\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "    end = time.time()\n",
    "    print(f\"time taken: {timedelta(seconds=end-start)}\")\n",
    "    return outlier_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to save outlier results\n",
    "def save_outlier_results(outlier_results, path):\n",
    "    outlier_df = pd.DataFrame(outlier_results,columns=[\"outlier_score\",'timestamp']) # convert to dataframe\n",
    "    outlier_df['timestamp'] = pd.to_datetime(outlier_df['timestamp']) # convert timestamp to datetime\n",
    "    outlier_df.set_index('timestamp',inplace=True) # set timestamp as index\n",
    "    outlier_df.to_csv(path) # save to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =========== MAIN ==========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations for my model\n",
    "# # define AE model parameters\n",
    "# lr = 1e-2         # learning rate\n",
    "# w_d = 1e-5        # weight decay\n",
    "# epochs = 1       # number of epochs\n",
    "# device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
    "\n",
    "# DAADS MODELS = [\"AE\", \"DAE\",\"PW-AE\"]\n",
    "MODELS = [\"AE\", \"AE\", \"DAE\", \"RRCF\", \"HST\", \"PW-AE\", \"xStream\", \"Kit-Net\", \"ILOF\"]\n",
    "CONFIGS = {\n",
    "    \"AE\": {\"lr\": 0.02, \"latent_dim\": 0.1},\n",
    "    \"DAE\": {\"lr\": 0.02},\n",
    "    \"PW-AE\": {\"lr\": 0.1},\n",
    "    \"OC-SVM\": {},\n",
    "    \"HST\": {\"n_trees\": 25, \"height\": 15},\n",
    "}\n",
    "AE_model = 'PW-AE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load data from pickle file\n",
    "# load_fpds_path = f\"../data/hauge/processed/featured_fpds_raw.pickle\"\n",
    "# featured_fpds = load_pickle(load_fpds_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load config file\n",
    "# with open('../utils/configs.json') as f:\n",
    "#     config = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for each trajectory, direction and intersection, run AE model\n",
    "# for trajectory in config['trajectories']:\n",
    "#     for direction in config['trajectories'][trajectory]:\n",
    "#         for intersection in config['trajectories'][trajectory][direction]:\n",
    "#             # print trajectory, direction, intersection\n",
    "#             print(f\"processing trajectory - {trajectory},  direction - {direction}, and intersection - {intersection} \")\n",
    "#             intersection_data = featured_fpds[trajectory][direction]['fpds'][intersection]\n",
    "#             intersection_data_flatten = prepare_input_for_PWAE(intersection_data) # flatten data\n",
    "#             df = pd.DataFrame(intersection_data_flatten, columns=['var'+str(i) for i in range(1, 13)]+['timestamp','Isanomaly'])\n",
    "#             train_data = df[['var'+str(i) for i in range(1, 13)]].to_dict('records')\n",
    "#             scores,total_time = test_then_train(dataset='OWRI',model=AE_model,seed=42,data = train_data,**CONFIGS.get(AE_model, {}))\n",
    "#             outlier_results = [[x,y] for x,y in zip(scores, df['timestamp'].to_list())]\n",
    "#             # AE_dataset = AutoEncoderDataset(intersection_data_flatten) # create AE dataset\n",
    "#             # outlier_results = AE_outlier_detection(AE_dataset, device, lr, w_d, epochs, intersection) # run AE model\n",
    "#             # save outlier scores to csv\n",
    "#             AE_score_save_path = f\"../results/hauge/outlier_scores/{AE_model}/{intersection}_{direction}.csv\"\n",
    "#             save_outlier_results(outlier_results, AE_score_save_path)\n",
    "#             print(f\"{intersection} done!\")\n",
    "#             print('-----------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from pickle file\n",
    "load_fpds_path= f\"../data/METR-LA/METR_OWRI/featured_fpds_raw.pickle\"\n",
    "featured_fpds = load_pickle(load_fpds_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AE for metr-la dataset\n",
    "for intersection in featured_fpds['fpds']:\n",
    "    # print intersection\n",
    "    print(f\"processing intersection - {intersection} \")\n",
    "    intersection_data = featured_fpds['fpds'][intersection]\n",
    "    intersection_data_flatten = prepare_input_for_PWAE(intersection_data) # flatten data\n",
    "    df = pd.DataFrame(intersection_data_flatten, columns=['var'+str(i) for i in range(1, 13)]+['timestamp','Isanomaly'])\n",
    "    train_data = df[['var'+str(i) for i in range(1, 13)]].to_dict('records')\n",
    "    df = df.sort_values(by='timestamp')\n",
    "    scores,total_time = test_then_train(dataset='OWRI',model=AE_model,seed=42,data = train_data,**CONFIGS.get(AE_model, {}))\n",
    "    outlier_results = [[x,y] for x,y in zip(scores, df['timestamp'].to_list())]\n",
    "    # AE_dataset = AutoEncoderDataset(intersection_data_flatten) # create AE dataset\n",
    "    # outlier_results = AE_outlier_detection(AE_dataset, device, lr, w_d, epochs, intersection) # run AE model\n",
    "    # save outlier scores to csv\n",
    "    AE_score_save_path = f\"../results/METR-LA/outlier_scores/{AE_model}/{intersection}.csv\"\n",
    "    save_outlier_results(outlier_results, AE_score_save_path)\n",
    "    print(f\"{intersection} done!\")\n",
    "    print('-----------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXTRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direction = \"North\"\n",
    "trajectory = \"T1\"\n",
    "intersection = \"K502\"\n",
    "intersection_data = featured_fpds[trajectory][direction][intersection]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total numbe of data points\n",
    "l = 0\n",
    "for i in range(7):\n",
    "    for j in range(24):\n",
    "        l += len(intersection_data[i][j][0])\n",
    "print(f\"total number of data points for intersection - {intersection} is {l}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare input for AEs\n",
    "intersection_data_flatten = prepare_input_for_AEs(intersection_data, intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AE_dataset = AutoEncoderDataset(intersection_data_flatten)\n",
    "AE_dataset.dim # feature vector dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(AE_dataset, batch_size=1, shuffle=False)\n",
    "# train_dataloader = DataLoader(AE_dataset, batch_size=1, shuffle=False, collate_fn=my_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if dataloader is working\n",
    "for i in train_dataloader:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model parameters\n",
    "lr = 1e-2         # learning rate\n",
    "w_d = 1e-5        # weight decay\n",
    "epochs = 1\n",
    "metrics = defaultdict(list)\n",
    "outlier_loss = []\n",
    "outlier_results = []\n",
    "device = 'mps' if torch.backends.mps.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AE(AE_dataset.dim)\n",
    "model.to(device)\n",
    "criterion = nn.MSELoss(reduction='mean')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr, weight_decay=w_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "start = time.time()\n",
    "for epoch in range(epochs):\n",
    "    ep_start = time.time()\n",
    "    running_loss = 0.0\n",
    "    for bx, data in enumerate(train_dataloader):\n",
    "        bt = data[0]\n",
    "        sensor_time = data[1][0]\n",
    "        sample = model(bt.to(device))\n",
    "        loss = criterion(bt.to(device), sample) # calculate loss for input and recreated output\n",
    "        outlier_loss.append(loss.item()) # append loss to list\n",
    "        outlier_results.append([loss.item(),sensor_time]) # append loss, time and intersection to list\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        # # print average loss for every 25% batches\n",
    "        # if bx % int(AE_dataset.__len__()/4) == 0:\n",
    "        #     print('[EPOCH] {}/{}\\t[BATCH] {}/{}\\t[LOSS] {}'.format(epoch+1,epochs,bx+1,AE_dataset.__len__(),running_loss/(bx+1)))\n",
    "    epoch_loss = running_loss/AE_dataset.__len__()\n",
    "    metrics['train_loss'].append(epoch_loss)\n",
    "    ep_end = time.time()\n",
    "    print('-----------------------------------------------')\n",
    "    print('[EPOCH] {}/{}\\n[LOSS] {}'.format(epoch+1,epochs,epoch_loss))\n",
    "    print('Epoch Complete in {}'.format(timedelta(seconds=ep_end-ep_start)))\n",
    "end = time.time()\n",
    "print('-----------------------------------------------')\n",
    "print('[{} Training Completed: {}]'.format(intersection, timedelta(seconds=end-start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1,1,figsize=(10,5))\n",
    "ax.set_title('Loss')\n",
    "ax.plot(outlier_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the top 5% of the loss values\n",
    "top_5_percent = np.percentile(outlier_loss, 97)\n",
    "top_5_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_threshold = 0.0\n",
    "upper_threshold = top_5_percent\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.title('Loss Distribution')\n",
    "sns.distplot(outlier_loss,bins=100,kde=True, color='blue')\n",
    "plt.axvline(upper_threshold, 0.0, 10, color='r')\n",
    "plt.axvline(lower_threshold, 0.0, 10, color='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxenplot(outlier_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_df = pd.DataFrame(outlier_results,columns=[intersection,'timestamp'])\n",
    "outlier_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_df['timestamp'] = pd.to_datetime(outlier_df['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make timestamp as index\n",
    "outlier_df.set_index('timestamp',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AE_score_save_path = f\"../results/hauge/outlier_scores/AE/{intersection}.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_df.to_csv(AE_score_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(outlier_df['timestamp'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_df['timestamp'] = outlier_df['timestamp'].apply(lambda x: x[0])\n",
    "outlier_df['intersection'] = outlier_df['intersection'].apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create seperate columns for each intersection\n",
    "outlier_df_intersection = outlier_df.pivot(index='timestamp', columns='intersection', values='loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(facecolor='w')\n",
    "sns.heatmap(outlier_df_intersection.corr())\n",
    "plt.title(\"Correlations for T1 North\")\n",
    "plt.xlabel('Intersection')\n",
    "plt.ylabel('Intersection')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_df = outlier_df_intersection.corr()\n",
    "corr_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments over correlated intersections\n",
    "- Except K198 all intersections are highly correlated\n",
    "- hard to get the intution as the feature vector is - **[fpds + hourofday+ weekofday + intersectoion]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each intersection, get the top 3 correlated intersections\n",
    "top_3_corr = corr_df.apply(lambda x: x.sort_values(ascending=False).index[1:4], axis=1)\n",
    "top_3_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OWRI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
