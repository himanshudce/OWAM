{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this notebook is an extension of the code - https://github.com/lucasczz/DAADS to run the autoencoder models and other outlier models\n",
    "# import libraries\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import scipy\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "import time\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "from datetime import timedelta\n",
    "\n",
    "# define random seeds for Neural Networks\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# ignore warnings jupyter notebook\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# importing from daads\n",
    "import sys\n",
    "# set path for the DAADS package\n",
    "sys.path.insert(0, os.getcwd().replace('notebooks', 'DAADS'))\n",
    "from tools.evaluate import aggregate_dataframe, test_then_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ========== FUNCTION =============="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare input for AEs, \n",
    "# input is combined list of all intersections (vector -> [fpds + hourofday+ weekofday + intersectoion]) for each week and each hour sorted based on time\n",
    "# output is -> list of feature vector and time for all interctions combined --> [[fpds + hourofday+ weekofday + intersectoion], time]\n",
    "def prepare_input_for_PWAE(intersection_data):\n",
    "    combined_fpds_for_AEs = []\n",
    "    for i in range(7):\n",
    "        for j in range(24):\n",
    "            for fpd,time_instance in zip(intersection_data[i][j][0],intersection_data[i][j][1]):\n",
    "                temp = fpd.astype(np.float32).tolist()\n",
    "                temp.extend([time_instance, 0])\n",
    "                combined_fpds_for_AEs.append(temp)\n",
    "\n",
    "    # combined_fpds_for_AEs = sorted(combined_fpds_for_AEs, key=lambda x:x[12])\n",
    "    return combined_fpds_for_AEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load data from pickle file\n",
    "def load_pickle(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to save outlier results\n",
    "def save_outlier_results(outlier_results, path):\n",
    "    outlier_df = pd.DataFrame(outlier_results,columns=[\"outlier_score\",'timestamp']) # convert to dataframe\n",
    "    outlier_df['timestamp'] = pd.to_datetime(outlier_df['timestamp']) # convert timestamp to datetime\n",
    "    outlier_df.set_index('timestamp',inplace=True) # set timestamp as index\n",
    "    outlier_df.to_csv(path) # save to csv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# =========== MAIN ==========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# provide the models you want to train, and their hyperparameters if needed for more reference go to - (https://github.com/lucasczz/DAADS)\n",
    "MODELS = [\"AE\", \"DAE\",\"PW-AE\", \"ILOF\", \"HST\"]\n",
    "# MODELS = [\"PW-AE\"]\n",
    "CONFIGS = {\n",
    "    \"AE\": {\"lr\": 0.01, \"latent_dim\": 0.2},\n",
    "    \"DAE\": {\"lr\": 0.01},\n",
    "    \"PW-AE\": {\"lr\": 0.01},\n",
    "    \"OC-SVM\": {},\n",
    "    \"HST\": {\"n_trees\": 25, \"height\": 15},\n",
    "}\n",
    "earths_movers_distance = True\n",
    "\n",
    "# use EMD or RMSE as metric for reconstruction error \n",
    "# to use this make necesary change in the DAADS code\n",
    "# the loss function for EMD is already added in the DAADS code but it needed to be updated when usind EMD\n",
    "# change the loss function in the DAADS code to the following\n",
    "# 1. go to \"OWRI/DAADS/IncrementalTorch/IncrementalTorch/anomaly.py\" and change loss_fn=\"emd\" in all the classes for which you want to use EMD\n",
    "# 2. the available loss functions are (File location - \"OWRI/DAADS/IncrementalTorch/IncrementalTorch/utils/module_finder\")- \n",
    "        # \"mse\": F.mse_loss,\n",
    "        # \"rmse\": rmse_loss,\n",
    "        # \"emd\": earth_mover_loss,\n",
    "        # \"mae\": F.l1_loss,\n",
    "        # \"smooth_mae\": F.smooth_l1_loss,\n",
    "        # \"bce\": F.binary_cross_entropy,\n",
    "        # \"ce\": F.cross_entropy,\n",
    "        # \"kld\": F.kl_div,\n",
    "        # \"huber\": F.huber_loss\n",
    "\n",
    "# 3. EMD is defined at the above location as - \n",
    "    # # earth mover's distance loss\n",
    "    # def earth_mover_loss(input, target, size_average=None, reduce=None, reduction=\"mean\"):\n",
    "    #     # Compute the distance matrix between the bins of the distributions\n",
    "    #     distance_matrix = cdist(np.arange(len(input)).reshape(-1, 1).astype('float64'), np.arange(len(target)).reshape(-1, 1).astype('float64'), metric='cityblock')\n",
    "    #     # Compute the EMD between the two distributions\n",
    "    #     emd_distance = emd(input.astype('float64'), target.astype('float64'), distance_matrix)\n",
    "    #     return emd_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load config file for hague\n",
    "with open('../utils/configs.json') as f:\n",
    "    config = json.load(f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---------------------------- Hague Outlier Processing ---------------------------- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare variable and load data from pickle file\n",
    "data_name = \"hague\"\n",
    "load_fpds_path = f\"../data/{data_name}/processed/featured_fpds_raw.pickle\"\n",
    "featured_fpds = load_pickle(load_fpds_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in MODELS:\n",
    "\n",
    "    print(f\"processing model - {model_name} \")\n",
    "    AE_model = model_name\n",
    "    data_name = data_name\n",
    "    load_fpds_path = f\"../data/{data_name}/processed/featured_fpds_raw.pickle\"\n",
    "    featured_fpds = load_pickle(load_fpds_path)\n",
    "    model_save_path = f\"../results/{data_name}/outlier_scores/{AE_model}\"\n",
    "    if earths_movers_distance:\n",
    "        model_save_path = model_save_path + \"_EMD\"\n",
    "\n",
    "    if not os.path.exists(model_save_path):\n",
    "        os.makedirs(model_save_path)\n",
    "\n",
    "    time_list = []\n",
    "    # for each trajectory, direction and intersection, run AE model\n",
    "    for trajectory in config['trajectories']:\n",
    "        for direction in config['trajectories'][trajectory]:\n",
    "            for intersection in config['trajectories'][trajectory][direction]:\n",
    "                # print trajectory, direction, intersection\n",
    "                print(f\"processing trajectory - {trajectory},  direction - {direction}, and intersection - {intersection} \")\n",
    "                intersection_data = featured_fpds[trajectory][direction]['fpds'][intersection]\n",
    "                intersection_data_flatten = prepare_input_for_PWAE(intersection_data) # flatten data\n",
    "                df = pd.DataFrame(intersection_data_flatten, columns=['var'+str(i) for i in range(1, 13)]+['timestamp','Isanomaly']) # convert to dataframe\n",
    "                train_data = df[['var'+str(i) for i in range(1, 13)]].to_dict('records') # convert to dict\n",
    "                df = df.sort_values(by='timestamp')\n",
    "                scores,total_time = test_then_train(train_data, dataset='OWRI',model=AE_model,seed=42,**CONFIGS.get(AE_model, {}))\n",
    "                time_list.append(total_time)\n",
    "                outlier_results = [[x,y] for x,y in zip(scores, df['timestamp'].to_list())]\n",
    "                # save outlier scores to csv\n",
    "                AE_score_save_path = os.path.join(model_save_path,f\"{intersection}_{direction}.csv\") # path to save outlier scores\n",
    "                save_outlier_results(outlier_results, AE_score_save_path)\n",
    "                # print(f\"{intersection} done!\")\n",
    "                print('-----------------------------------------------')\n",
    "\n",
    "\n",
    "    # save time to pickle file\n",
    "    time_save_path = os.path.join(model_save_path,'instance_train_time_seconds.pkl') # path to save time\n",
    "    with open(time_save_path, 'wb') as f:\n",
    "        pickle.dump(time_list, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---------------------------- METR-LA Outloer Processing ---------------------------- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare variable and load data from pickle file\n",
    "data_name = \"METR-LA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AE for metr-la dataset\n",
    "# an input vector is a 12-dimensional vector of the traffic data of the last 12 points (i.e., 5 minutes)\n",
    "# input is modified according to the daads implementation\n",
    "# use debug to print out the necessary steps of the input and output for better understanding\n",
    "\n",
    "for model_name in MODELS:\n",
    "\n",
    "    print(f\"processing model - {model_name} \")\n",
    "    AE_model = model_name\n",
    "    data_name = data_name\n",
    "    load_fpds_path = f\"../data/{data_name}/processed/featured_fpds_raw.pickle\"\n",
    "    featured_fpds = load_pickle(load_fpds_path)\n",
    "    model_save_path = f\"../results/{data_name}/outlier_scores/{AE_model}\"\n",
    "    if earths_movers_distance:\n",
    "        model_save_path = model_save_path + \"_EMD\"\n",
    "\n",
    "    if not os.path.exists(model_save_path):\n",
    "        os.makedirs(model_save_path)\n",
    "\n",
    "    time_list = []\n",
    "    for intersection in featured_fpds['fpds']:\n",
    "        # print intersection\n",
    "        # print(f\"processing intersection - {intersection} \")\n",
    "        intersection_data = featured_fpds['fpds'][intersection]\n",
    "        intersection_data_flatten = prepare_input_for_PWAE(intersection_data) # flatten data\n",
    "        df = pd.DataFrame(intersection_data_flatten, columns=['var'+str(i) for i in range(1, 13)]+['timestamp','Isanomaly']) # convert to dataframe\n",
    "        train_data = df[['var'+str(i) for i in range(1, 13)]].to_dict('records') # convert to dict\n",
    "        df = df.sort_values(by='timestamp')\n",
    "        scores,total_time = test_then_train(train_data, dataset='OWRI',model=AE_model,seed=42,**CONFIGS.get(AE_model, {}))\n",
    "        time_list.append(total_time)\n",
    "        outlier_results = [[x,y] for x,y in zip(scores, df['timestamp'].to_list())]\n",
    "        # save outlier scores to csv\n",
    "        AE_score_save_path = os.path.join(model_save_path, intersection+'.csv') # path to save outlier scores\n",
    "        save_outlier_results(outlier_results, AE_score_save_path)\n",
    "        # print(f\"{intersection} done!\")\n",
    "        print('-----------------------------------------------')\n",
    "\n",
    "\n",
    "    # save time to pickle file\n",
    "    time_save_path = os.path.join(model_save_path,'instance_train_time_seconds.pkl') # path to save time\n",
    "    with open(time_save_path, 'wb') as f:\n",
    "        pickle.dump(time_list, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ---------------------------- PEMS-BAY Outlier Processing ---------------------------- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare variable and load data from pickle file\n",
    "data_name = \"PEMS-BAY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AE for metr-la dataset\n",
    "# an input vector is a 12-dimensional vector of the traffic data of the last 12 points (i.e., 5 minutes)\n",
    "\n",
    "for model_name in MODELS:\n",
    "\n",
    "    print(f\"processing model - {model_name} \")\n",
    "    AE_model = model_name\n",
    "    data_name = data_name\n",
    "    load_fpds_path = f\"../data/{data_name}/processed/featured_fpds_raw.pickle\"\n",
    "    featured_fpds = load_pickle(load_fpds_path)\n",
    "    model_save_path = f\"../results/{data_name}/outlier_scores/{AE_model}\"\n",
    "    if earths_movers_distance:\n",
    "        model_save_path = model_save_path + \"_EMD\"\n",
    "\n",
    "    if not os.path.exists(model_save_path):\n",
    "        os.makedirs(model_save_path)\n",
    "\n",
    "    time_list = []\n",
    "    for intersection in featured_fpds['fpds']:\n",
    "        # print intersection\n",
    "        # print(f\"processing intersection - {intersection} \")\n",
    "        intersection_data = featured_fpds['fpds'][intersection]\n",
    "        intersection_data_flatten = prepare_input_for_PWAE(intersection_data) # flatten data\n",
    "        df = pd.DataFrame(intersection_data_flatten, columns=['var'+str(i) for i in range(1, 13)]+['timestamp','Isanomaly']) # convert to dataframe\n",
    "        train_data = df[['var'+str(i) for i in range(1, 13)]].to_dict('records') # convert to dict\n",
    "        df = df.sort_values(by='timestamp')\n",
    "        scores, total_time = test_then_train(train_data, dataset='OWRI',model=AE_model,seed=42,**CONFIGS.get(AE_model, {}))\n",
    "        time_list.append(total_time)\n",
    "        outlier_results = [[x,y] for x,y in zip(scores, df['timestamp'].to_list())]\n",
    "        # save outlier scores to csv\n",
    "        AE_score_save_path = os.path.join(model_save_path, str(intersection)+'.csv') # path to save outlier scores\n",
    "        save_outlier_results(outlier_results, AE_score_save_path)\n",
    "        # print(f\"{intersection} done!\")\n",
    "        print('-----------------------------------------------')\n",
    "\n",
    "\n",
    "    # save time to pickle file\n",
    "    time_save_path = os.path.join(model_save_path,'instance_train_time_seconds.pkl') # path to save time\n",
    "    with open(time_save_path, 'wb') as f:\n",
    "        pickle.dump(time_list, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----------- END --------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OWRI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
